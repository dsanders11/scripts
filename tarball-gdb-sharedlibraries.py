#!/usr/bin/env python

# Copyright 2013 David Sanders

"""Script for packaging up required shared libraries into a tarball for offsite
core dump analysis

The produced tarball maintains the directory structure of the shared libraries
used by the core dump, as seen from the root directory.

When using GDB to analyze the core dump, use the following commands:

(gdb) set solib-absolute-prefix /dev/null/
(gdb) nosharedlibrary
(gdb) set solib-search-path <path-to-libs>


NOTE: This script will only work with GDB v7.0 and above due to using the Python
API, and Python 2.5 and above

"""

import textwrap
import tarfile
import subprocess
import platform
import sys
import os
import re

from optparse import OptionParser

def get_process_name_from_coredump(filename):
    """Returns the name of the process which the core dump is from
    
    If this fails (such as the file not being a core dump) then a RuntimeError
    is raised
    
    """

    gdb_process = subprocess.Popen("gdb --batch --core {0}".format(filename),
                                   shell=True,
                                   stdout=subprocess.PIPE, 
                                   stderr=subprocess.PIPE)

    # wait for the process to terminate
    output, error_output = gdb_process.communicate()

    assert gdb_process.returncode == 0

    match = re.search("Core was generated by `(\S+)", output)

    if match is None:
        raise RuntimeError("Couldn't determine process name for coredump")
    else:
        return match.groups()[0]

def get_coredump_shared_libraries(core_filename):
    """Returns a list of absolute paths to shared libraries from the core dump
    
    This will also include the path to the binary which the core dump is from.
    
    """

    # First figure out what process created the core dump
    process_name = get_process_name_from_coredump(core_filename)

    # Next grab the list of shared libraries
    cmd_parts = [
        "gdb {0} -ex ".format(process_name),
        "\"python print 'SO LIST: {0}'",
        ".format([obj.filename for obj in gdb.objfiles()])\" ",
        "--batch --core={0}".format(core_filename)
    ]

    cmd = ''.join(cmd_parts)

    gdb_process = subprocess.Popen(cmd, shell=True,
                                   stdout=subprocess.PIPE, 
                                   stderr=subprocess.PIPE)

    # wait for the process to terminate
    output, error_output = gdb_process.communicate()

    assert gdb_process.returncode == 0

    shared_libraries = []

    for line in output.split('\n'):
        #print line
        if line.startswith("SO LIST: "):
            shared_libraries = line[9:].strip("[]").replace("'", '').split(", ")

    # Only return valid files. Hopefully all of the shared libraries are still
    # on this system. This line is mainly to strip out the occasional lines of
    # 'system-supplied DSO' in the output, but also serves to weed out bad files
    shared_libraries = [lib for lib in shared_libraries if os.path.isfile(lib)]

    return shared_libraries

def tar_coredump_shared_libraries(core_filename, tar_filename):
    """Creates a tarball of the shared libraries found in the core dump, as well
    as the process binary
    
    """

    shared_libraries = get_coredump_shared_libraries(core_filename)

    tarball = tarfile.open(tar_filename, "w:gz")

    for lib in shared_libraries:
        if os.path.islink(lib):
            real_path = os.path.realpath(lib)
            previous_symlink = lib
            symlink = lib

            # Walk the symlinks, adding each to the tarball
            # until we reach a real file
            while symlink != real_path:
                tarball.add(symlink, arcname=symlink)

                previous_symlink = symlink
                symlink = os.readlink(previous_symlink)
                
                # os.readlink might return a relative path
                if not os.path.exists(symlink):
                    symlink = os.path.join(os.path.dirname(previous_symlink),
                                           symlink)

            tarball.add(real_path, arcname=real_path)
        else:
            tarball.add(lib, arcname=lib)

    tarball.close()

if __name__ == '__main__':
    usage_str = """\
        Usage:
            %prog [core] [output]
            %prog [core] --list"""
    
    parser = OptionParser(usage=textwrap.dedent(usage_str),
                          version="%prog 0.1")
    parser.add_option("--list", help="list the files and exit",
                      action="store_true", dest="list_files", default=False)

    (options, args) = parser.parse_args()

    if options.list_files:
        if len(args) != 1:
            parser.parse_args(["--help"])

        core_filename = args[0]

        if not os.path.isfile(core_filename):
            sys.stderr.write("{0}: No such file\n".format(core_filename))
            exit(1)

        for lib in get_coredump_shared_libraries(core_filename):
            print lib
    else:
        if len(args) != 2:
            parser.parse_args(["--help"])

        core_filename, output_filename = args

        if not os.path.isfile(core_filename):
            sys.stderr.write("{0}: No such file\n".format(core_filename))
            exit(1)

        if not output_filename.endswith(".tar.gz"):
            err_msg = ("WARNING: Output file will be a gzipped tarball, "
                       "but filename doesn't end in .tar.gz\n")
            sys.stderr.write(err_msg)

        tar_coredump_shared_libraries(core_filename, output_filename)